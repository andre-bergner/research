NEXT STEPS
• 2D ARNN benchmark
• start writing chapter on 2-osc:
    • situation linear vs nonlinear NN
    • nice pictures of 2-Tori etc.
• validation/test set --> measure success of prediction
• compare models (arnn, parnn, tae) with noisy input

• GAN-(V)AE:
   1. MNIST AE (clean up)
   2. MNIST VAE
   3. GAN-(V)AE: use GAE-loss on top of (V)AE 
   4. GAN-(V)AE for drum samples


• move big block to own TODO/NOTES file
• dwt tool

• AE+TAE: train parallel AE and TAE using hidden mapping phi: code[n] -> code[n+1]
• 2D ARNN
• try U-Net skip connections (with additional layer in between?)
• split from vs-file another comparison: model vs model2 vs AE+TAE
• try VAE

• Idea: Problem might be statistical: low values are more often samples then high ones (log-scale)
  → try: linear fade
  → try: sinusoidal amp-mod
  → try: normalize input and output, apply in predict as well 
  → try: normalized version with linear TAE --> look at embedding for spiral
  → counter-arg: linear signal?
• train normal AE on decaying signal
• use shifts of 50% or 100%, average with interleaved independent predictions
• use CNN instead of dense
• minimal repro: 'linear' spiral vs 'nonlinear' spiral
• wavenet sigmoid? tanh*sigma?
• study nsynth code --> understand integration of VAE
• wavelet AE
• compare 2D TAE vs ARNN


TODO / IDEAS
✔ concatenate with x-fade
✔ try different shifts
✔ try different frame_size's
✔ DAE --> works
✔ second-order prediction --> train model and model^2 with frame[+1] and frame[+2], respectively
• train two signals simultiously (in same AE) (two attractors)
• DAE + L2 loss
✔ plot code
• improve prediction steps
• use just next sample from prediction
• compare against LSTM-RNN
• use AE-approach + gready pre-training
• plot distances
• try out two coupled TAEs working on different time scales, coupling: common latent space or master-slave
✔ try generating ginzburg landau (use 1d-conv)
• try generating images/texture
• DAE in second order prediction?
• use shifts of 50% or 100%, average with interleaved independent predictions
• use CNN instead of dense
✔ contracting AE
• start from random position
    --> show that manifold is attraktor
    --> compare against ARNN (which hopefully blows up)
• scan for good model for Lorenz to be used in the paper
• strong noise and low sampling rate
• learning parameter space (example Lorenz which needs 3 dimensions in latent space)
   --> add one dimension to latent space
   --> train several indpendent time-series' generated for different paramters simultanously
   --> open problem: how to force the network to put the learned pararameter in the addition dimension?
• impact of latent_space size for noisy system!
• use distance between prediction and original as stopping/quality metric
• test with Rössler & Lorenz signal
• compare against plain nonlineae-AR-networks
• "average" over several succesfully learned models --> common structure?
• impact of latent dim on prediction: min size, stability of to big?
• measure:
  • impact of timestep
  • impact of latent dim
  • impact of noise
  • impact of batch-size
• apply 2d-TAE in wavelet-domain
• try to synchroinze to trained models, e.g. two Rössler oscillators
• train more models: Laser, Rössler, Bisrtin Neurons, Chua...
• connection between signal time-scale and step-size
• batch normalization
• get 'normal' (conv-)autoencoder for signals working
• try learning noisy time series (long time)
• try to add wavelet-ae outer ring
• try loss functions: fft, separate lopass & hipass filters
• try adding noise (denoising timeshift AE)
• try to increase frame_size using deep-conv with downsampling
• try strided prediction (Taken's theorem) --> random sampling?
• hyper plane time shift AE: learns to predict several signals
  --> i.e. add the classic AE feature of learning disjoint entities that lie on a common manifold
  --> additional to the manifold of the flow learn neighboring manifolds in the space of dynamical systems
